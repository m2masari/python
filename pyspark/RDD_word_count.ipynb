{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Word Count\n",
    "<br>\n",
    "<font size=4,font style=arial>\n",
    "Bu yazıda ki amacımız;<br> \n",
    "    -Metnin RDD olarak okunması<br> \n",
    "    -Noktalama işaretlerinin çıkarılması<br>\n",
    "    -Büyük harflerin küçük harf yapılması (Aksi takdirde Spark iki farklı kelime gibi algılıyor. Örnek Computer ve computer,iki farklı kelime group by yapıp toplandığında tek kelime altında toplanmıyor)<br>\n",
    "    -Kelimelere ayırıp, aynı kelimeleri group by ile toplayıp, büyükten küçüğe sort yapıp, en fazla geçen kelimenin bulunması<br>\n",
    "<br>\n",
    "<br>\n",
    "    Metnimizi RDD olarak okuyalım ve text adında kaydedelim. (RDD 'ler unstructure bir yapıya sahip olduklarından, word count'lar için idealdir.)\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=sc.parallelize([\"#$Apache\\Hadoop* -and- APache -./:;<=>? Spark: are +well-known@examples:;<=> of Big $$$data%%% processing{|} systems.\\\n",
    "   Hadoop$%&$%&$%& and \\\\SpArk\\\\]^ are<=> designed'' for^^^^ distributed *+,-./:;<=>?@[\\processing of large#$%&\\'()*+,-./:;<\\\n",
    "   =>?@[\\data$%& sets@@ across( {|}~clusters of +,-.ComputERS\\'()*+,-./:;<.apache++ SpArk is%&  open +,-.source and one^^ of the Most\\\n",
    "   FAMOUS;<= Big DATA$% PROCESSING systems\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3,font style=arial>\n",
    "<br>\n",
    "SparkContext'imizi ve SparkSession'ımızı oluşturalım\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3,font style=arial>\n",
    "<br>\n",
    "İhtiyacımız olan fonksiyonları import edelim\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3,font style=arial>\n",
    "<br>\n",
    "Aşağıda ki fonksiyon yardımıyla; büyük harfleri küçültelim ve noktalama işaretlerini ve gereksiz karakterleri metinden çıkaralım<br>\n",
    "Not:Lower harfleri küçültüyor<br>\n",
    "Not:Replace noktalama işaretlerini çıkarıyor<br>\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noktalama_kucuk_harf(x):\n",
    "    punc='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    lowercased_str = x.lower()\n",
    "    for ch in punc:\n",
    "        lowercased_str = lowercased_str.replace(ch, ' ')\n",
    "    return lowercased_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3,font style=arial>\n",
    "<br>\n",
    "Textimizin yeni hali aşağıdadır. Bütün noktalama işaretlerinden arındırılmış ve harfler küçülmüş durumda.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  apache hadoop   and  apache           spark  are  well known examples      of big    data    processing    systems    hadoop          and  spark    are    designed   for     distributed                processing of large                         data    sets   across      clusters of     computers             apache   spark is    open     source and one   of the most   famous    big data   processing systems']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd = text.map(noktalama_kucuk_harf)\n",
    "text_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3,font style=arial>\n",
    "<br>\n",
    "Aşağıda Flatmap transformation'ı ile texti kelimelere böldük. Herbir transformations'dan sonra RDD'ler yeni bir RDD'ye atanır. Mevcut RDD değiştirilemez(Immutable).  \n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word_flatmap=text_rdd.flatMap(lambda s:s.split (\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3,font style=arial>\n",
    "<br>\n",
    "Take bir action'dır ve RDD'nin içeriğini görmemizi sağlar.(5 tanesini listeleyelim. Eğer ki istenirse text_word_flatmap.collect() kullanılarak RDD'nin tamamı listeleneblir fakat collect action'ı büyük veri için memory sıkıntısı \n",
    " yaratabilir.)\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', 'apache', 'hadoop', '']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_word_flatmap.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4,font style=arial>\n",
    "Textimizdeki boşluk sayısı çok (arındırdığımz noktalama işaretlerini boşluklar ile değiştirdik). Bu boşlukları filitreleyelim\n",
    "    </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word_flatmap1 = text_word_flatmap.filter(lambda x: len(x)>0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4,font style=arial>\n",
    "<br>\n",
    "Boşluklar filitrelendikten sonra metinde geçen kelimeleri listeleyelim\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apache',\n",
       " 'hadoop',\n",
       " 'and',\n",
       " 'apache',\n",
       " 'spark',\n",
       " 'are',\n",
       " 'well',\n",
       " 'known',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'big',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'hadoop',\n",
       " 'and',\n",
       " 'spark',\n",
       " 'are',\n",
       " 'designed',\n",
       " 'for',\n",
       " 'distributed',\n",
       " 'processing',\n",
       " 'of',\n",
       " 'large',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'across',\n",
       " 'clusters',\n",
       " 'of',\n",
       " 'computers',\n",
       " 'apache',\n",
       " 'spark',\n",
       " 'is',\n",
       " 'open',\n",
       " 'source',\n",
       " 'and',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'famous',\n",
       " 'big',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'systems']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_word_flatmap1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4,font style=arial>\n",
    "<br>\n",
    "İhtiyacımız olmayan kelimeleri metinden çıkaralım. Çıkaracağımız kelimeler={'is','and','are','of','and','for','the'}\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cikacak_kelimeler={'is','and','are','of','and','for','the'}\n",
    "text_word_flatmap2 = text_word_flatmap1.filter(lambda x: x not in cikacak_kelimeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4,font style=arial>\n",
    "<br>\n",
    "Map transformation'ı ile kelimelerin yanına 1 getirelim(daha sonra bu kelimeleri toplayacağız)\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_map=text_word_flatmap2.map(lambda s:(s,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apache', 1),\n",
       " ('hadoop', 1),\n",
       " ('apache', 1),\n",
       " ('spark', 1),\n",
       " ('well', 1),\n",
       " ('known', 1),\n",
       " ('examples', 1),\n",
       " ('big', 1),\n",
       " ('data', 1),\n",
       " ('processing', 1),\n",
       " ('systems', 1),\n",
       " ('hadoop', 1),\n",
       " ('spark', 1),\n",
       " ('designed', 1),\n",
       " ('distributed', 1),\n",
       " ('processing', 1),\n",
       " ('large', 1),\n",
       " ('data', 1),\n",
       " ('sets', 1),\n",
       " ('across', 1),\n",
       " ('clusters', 1),\n",
       " ('computers', 1),\n",
       " ('apache', 1),\n",
       " ('spark', 1),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('one', 1),\n",
       " ('most', 1),\n",
       " ('famous', 1),\n",
       " ('big', 1),\n",
       " ('data', 1),\n",
       " ('processing', 1),\n",
       " ('systems', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4,font style=arial>\n",
    "<br>\n",
    "Şimdi kelimeleri group by yapıp toplayalım\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reduce=text_map.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apache', 3),\n",
       " ('hadoop', 2),\n",
       " ('spark', 3),\n",
       " ('well', 1),\n",
       " ('known', 1),\n",
       " ('examples', 1),\n",
       " ('big', 2),\n",
       " ('data', 3),\n",
       " ('processing', 3),\n",
       " ('systems', 2),\n",
       " ('designed', 1),\n",
       " ('distributed', 1),\n",
       " ('large', 1),\n",
       " ('sets', 1),\n",
       " ('across', 1),\n",
       " ('clusters', 1),\n",
       " ('computers', 1),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('one', 1),\n",
       " ('most', 1),\n",
       " ('famous', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_reduce.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4,font style=arial>\n",
    "<br>\n",
    "Büyükten küçüğe sort yapıp en fazla geçen 5 kelimeyi bulalım.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apache', 3), ('spark', 3), ('data', 3), ('processing', 3), ('hadoop', 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_reduce.sortBy(lambda a: a[1],False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3,font style=arial>\n",
    "<br>\n",
    "Görüldüğü üzere metinde en fazla apache,spark,data ve processing kelimeleri 3'er defa geçiyor. Metinin büyük veriden bahsettiği kesin:)\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3,font style=arial>\n",
    "<br>\n",
    "Eğer ki istenirse baş harfleri aynı olan kelimelerde group by yapılabilir. Baş harfi aynı olan kelimeler hangileridir bir göz atalım\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "basharf = text_word_flatmap2.groupBy(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', ['apache', 'apache', 'across', 'apache'])\n",
      "('h', ['hadoop', 'hadoop'])\n",
      "('s', ['spark', 'systems', 'spark', 'sets', 'spark', 'source', 'systems'])\n",
      "('w', ['well'])\n",
      "('k', ['known'])\n",
      "('e', ['examples'])\n",
      "('b', ['big', 'big'])\n",
      "('d', ['data', 'designed', 'distributed', 'data', 'data'])\n",
      "('p', ['processing', 'processing', 'processing'])\n",
      "('l', ['large'])\n",
      "('c', ['clusters', 'computers'])\n",
      "('o', ['open', 'one'])\n",
      "('m', ['most'])\n",
      "('f', ['famous'])\n"
     ]
    }
   ],
   "source": [
    "for t in basharf.collect():\n",
    "    print((t[0],[i for i in t[1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
